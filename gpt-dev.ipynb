{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efbedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘Mahabharata.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the text\n",
    "!wget -nc https://huggingface.co/datasets/aaru2330/Mahabharath/resolve/main/Mahabharata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f925e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file\n",
    "with open('Mahabharata.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bf9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of dataset in characters: 14921047\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caedbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Complete Mahabharata in English\n",
      "The Mahabharata\n",
      "of\n",
      "Krishna-Dwaipayana Vyasa\n",
      "\n",
      "BOOK 1\n",
      "ADI PARVA\n",
      "Translated into English Prose from the Original Sanskrit Text by Kisari Mohan Ganguli [1883-1896]\n",
      "Scanned at sacred-texts.com, 2003. Proofed at Distributed Proofing, Juliet Sutherland, Project Manager. Additional proofing\n",
      "and formatting at sacred-texts.com, by J. B. Hare.\n",
      "TRANSLATOR'S PREFACE\n",
      "The object of a translator should ever be to hold the mirror upto his author. That being so, his chief duty is to represent so far as\n",
      "practicable the manner in which his author's ideas have been expressed, retaining if possible at the sacrifice of idiom and taste\n",
      "all the peculiarities of his author's imagery and of language as well. In regard to translations from the Sanskrit, nothing is easier\n",
      "than to dish up Hindu ideas, so as to make them agreeable to English taste. But the endeavour of the present translator has been\n",
      "to give in the following pages as literal a rendering as possible of the great wo\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfea7b",
   "metadata": {},
   "source": [
    "# Tokenzing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddac8610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\f !\"#&'(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz—\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Let observe the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92545d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 65, 2, 57, 74, 66, 77, 70]\n",
      "hi arjun\n"
     ]
    }
   ],
   "source": [
    "# Mapping characters into integers for encoding/decoding\n",
    "# character level tokenizers\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encode: string --> list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: list of integers --> string\n",
    "\n",
    "print(encode(\"hi arjun\"))\n",
    "print(decode(encode(\"hi arjun\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da11c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cea3d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MiniGPT/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14921047]) <built-in method type of Tensor object at 0x7b454070a300>\n",
      "tensor([74, 67,  2,  ...,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the entire text using pytorch\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[1000:]) # GPT view of the 1000 chars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba26aa",
   "metadata": {},
   "source": [
    "# Training the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589dfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/validation sets\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9682c",
   "metadata": {},
   "source": [
    "Block Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262cc2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([45, 64, 61,  2, 28, 71, 69, 72, 68])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the training example length sent to the model \n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d6642",
   "metadata": {},
   "source": [
    "In this single block of 9 characters, we have 8 individual examples packed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7645b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([45]), the target is: 64\n",
      "when the input is tensor([45, 64]), the target is: 61\n",
      "when the input is tensor([45, 64, 61]), the target is: 2\n",
      "when the input is tensor([45, 64, 61,  2]), the target is: 28\n",
      "when the input is tensor([45, 64, 61,  2, 28]), the target is: 71\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71]), the target is: 69\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71, 69]), the target is: 72\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71, 69, 72]), the target is: 68\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when the input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765514",
   "metadata": {},
   "source": [
    "Batch Sizes for GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af56ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[65, 70, 60, 77, 68, 63, 61,  2],\n",
      "        [ 2, 77, 70, 76, 71,  2, 64, 65],\n",
      "        [79, 57, 74, 74, 65, 71, 74, 75],\n",
      "        [61, 60, 12,  2, 45, 64, 61,  2]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70, 60, 77, 68, 63, 61,  2, 65],\n",
      "        [77, 70, 76, 71,  2, 64, 65, 69],\n",
      "        [57, 74, 74, 65, 71, 74, 75,  2],\n",
      "        [60, 12,  2, 45, 64, 61,  2, 69]])\n",
      "----------\n",
      "When the input is: [65], the target is: 70\n",
      "When the input is: [65, 70], the target is: 60\n",
      "When the input is: [65, 70, 60], the target is: 77\n",
      "When the input is: [65, 70, 60, 77], the target is: 68\n",
      "When the input is: [65, 70, 60, 77, 68], the target is: 63\n",
      "When the input is: [65, 70, 60, 77, 68, 63], the target is: 61\n",
      "When the input is: [65, 70, 60, 77, 68, 63, 61], the target is: 2\n",
      "When the input is: [65, 70, 60, 77, 68, 63, 61, 2], the target is: 65\n",
      "When the input is: [2], the target is: 77\n",
      "When the input is: [2, 77], the target is: 70\n",
      "When the input is: [2, 77, 70], the target is: 76\n",
      "When the input is: [2, 77, 70, 76], the target is: 71\n",
      "When the input is: [2, 77, 70, 76, 71], the target is: 2\n",
      "When the input is: [2, 77, 70, 76, 71, 2], the target is: 64\n",
      "When the input is: [2, 77, 70, 76, 71, 2, 64], the target is: 65\n",
      "When the input is: [2, 77, 70, 76, 71, 2, 64, 65], the target is: 69\n",
      "When the input is: [79], the target is: 57\n",
      "When the input is: [79, 57], the target is: 74\n",
      "When the input is: [79, 57, 74], the target is: 74\n",
      "When the input is: [79, 57, 74, 74], the target is: 65\n",
      "When the input is: [79, 57, 74, 74, 65], the target is: 71\n",
      "When the input is: [79, 57, 74, 74, 65, 71], the target is: 74\n",
      "When the input is: [79, 57, 74, 74, 65, 71, 74], the target is: 75\n",
      "When the input is: [79, 57, 74, 74, 65, 71, 74, 75], the target is: 2\n",
      "When the input is: [61], the target is: 60\n",
      "When the input is: [61, 60], the target is: 12\n",
      "When the input is: [61, 60, 12], the target is: 2\n",
      "When the input is: [61, 60, 12, 2], the target is: 45\n",
      "When the input is: [61, 60, 12, 2, 45], the target is: 64\n",
      "When the input is: [61, 60, 12, 2, 45, 64], the target is: 61\n",
      "When the input is: [61, 60, 12, 2, 45, 64, 61], the target is: 2\n",
      "When the input is: [61, 60, 12, 2, 45, 64, 61, 2], the target is: 69\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2026)\n",
    "batch_size = 4 # independent sequences processed parallely\n",
    "block_size = 8 # Maximum context length of predicitons\n",
    "\n",
    "def get_batch(split):\n",
    "    # generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----------')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, : t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the input is: {context.tolist()}, the target is: {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9e56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65, 70, 60, 77, 68, 63, 61,  2],\n",
      "        [ 2, 77, 70, 76, 71,  2, 64, 65],\n",
      "        [79, 57, 74, 74, 65, 71, 74, 75],\n",
      "        [61, 60, 12,  2, 45, 64, 61,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a0deb",
   "metadata": {},
   "source": [
    "# Let us start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607676cb",
   "metadata": {},
   "source": [
    "###  Defining the neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83bb815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 84])\n",
      "tensor(4.8717, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\n",
      "m`D,h9.i_Tdxa5aC3Bk0eE]\f3eamQIFrSqZQq`]qtUqSgEokos!CnT0Mom`#,e?zYV#KpL`fF(OFclN-Z\"59`4Z\\GX,,qs,ds]\f\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BiGramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        # Basically we are passing the input idx to get the logits for next token prediction\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) = (batch_size, time = block_size, channel = vocab_size)\n",
    "\n",
    "        if targets is None: # If no targets provided, we are in generation mode\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape the logits and targets to calculate the loss as CrossEntropyLoss expects (N, C) and (N,) shape respectively\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, C) # last element in time dimension\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) # single prediciton in each batch\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "m = BiGramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # [0] to get the first batch element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814e3f2",
   "metadata": {},
   "source": [
    "### Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff5b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch optimizer and train the model\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c36ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 4.889804840087891\n",
      "Loss at step 500: 4.363897323608398\n",
      "Loss at step 1000: 3.8658130168914795\n",
      "Loss at step 1500: 3.575252056121826\n",
      "Loss at step 2000: 3.1285407543182373\n",
      "Loss at step 2500: 3.0089097023010254\n",
      "Loss at step 3000: 2.8657565116882324\n",
      "Loss at step 3500: 2.661139488220215\n",
      "Loss at step 4000: 2.683713912963867\n",
      "Loss at step 4500: 2.607184410095215\n",
      "Loss at step 5000: 2.4870002269744873\n",
      "Loss at step 5500: 2.4521849155426025\n",
      "Loss at step 6000: 2.5520248413085938\n",
      "Loss at step 6500: 2.455198049545288\n",
      "Loss at step 7000: 2.404175281524658\n",
      "Loss at step 7500: 2.4161956310272217\n",
      "Loss at step 8000: 2.3851940631866455\n",
      "Loss at step 8500: 2.350325345993042\n",
      "Loss at step 9000: 2.4114227294921875\n",
      "Loss at step 9500: 2.56941294670105\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Loss at step {step}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22dded16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "surof. stioffys I Drtherot wifof wiomeathrsahangoldglan? tthecovatherf. Rale hatr frbuly Kavas.\"\n",
      "b6.Federnd-wh s witesawhes ble o is, ald ara, a, s, otmarsecucind. f\n",
      "be, NqSEComemice by\n",
      "F\"in oupon fowild by-inge.em wibs l ctrthofrende. Saulisscceenthen s ispestheees tid, whmpred ondf wazlllis Thiots Whilorive, r LVites d kend o!\"\n",
      "ine horel-dhaind t\\543\n",
      "K2EGaelle Aplourse. irto o patof\n",
      "withitond sonin wrn s tr s, tetoleo pr ingn acean by h med pr his o gsshanennd s bonvaifo malgronthean this ty o\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist())) # ["
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282b719",
   "metadata": {},
   "source": [
    "# Self-Attention Block\n",
    "The trick is matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed43d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2026)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a5676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[b, t] = mean_{i<=t} x[b, i]\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C) includes the t-th element\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c9633d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839,  0.7296],\n",
       "        [ 0.6242,  0.1225],\n",
       "        [-0.0314, -0.7410],\n",
       "        [-1.3574, -0.0220],\n",
       "        [-0.5450,  0.2193],\n",
       "        [-1.4955,  0.3433],\n",
       "        [-0.1916, -0.9030],\n",
       "        [ 0.9982,  0.0361]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3e6ca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839,  0.7296],\n",
       "        [ 0.2201,  0.4261],\n",
       "        [ 0.1363,  0.0370],\n",
       "        [-0.2371,  0.0223],\n",
       "        [-0.2987,  0.0617],\n",
       "        [-0.4982,  0.1086],\n",
       "        [-0.4544, -0.0359],\n",
       "        [-0.2728, -0.0269]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85fd3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x # (#B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25c433",
   "metadata": {},
   "source": [
    " > Note: Even though wei is (T, T) and x is (B, T, C), python will transform (T, T) --> (B, T, T). Batch matrix multiplication was done in a weighted fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "758ebdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1839,  0.7296],\n",
       "         [ 0.2201,  0.4261],\n",
       "         [ 0.1363,  0.0370],\n",
       "         [-0.2371,  0.0223],\n",
       "         [-0.2987,  0.0617],\n",
       "         [-0.4982,  0.1086],\n",
       "         [-0.4544, -0.0359],\n",
       "         [-0.2728, -0.0269]]),\n",
       " tensor([[-0.1839,  0.7296],\n",
       "         [ 0.2201,  0.4261],\n",
       "         [ 0.1363,  0.0370],\n",
       "         [-0.2371,  0.0223],\n",
       "         [-0.2987,  0.0617],\n",
       "         [-0.4982,  0.1086],\n",
       "         [-0.4544, -0.0359],\n",
       "         [-0.2728, -0.0269]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0] # Identical batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e801cf5",
   "metadata": {},
   "source": [
    "> Using softmax, we do weighted aggregation of lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f6c748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # (#B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01def175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839,  0.7296],\n",
       "        [ 0.2201,  0.4261],\n",
       "        [ 0.1363,  0.0370],\n",
       "        [-0.2371,  0.0223],\n",
       "        [-0.2987,  0.0617],\n",
       "        [-0.4982,  0.1086],\n",
       "        [-0.4544, -0.0359],\n",
       "        [-0.2728, -0.0269]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79552871",
   "metadata": {},
   "source": [
    "Self Attention\n",
    "\n",
    "Every single token will emit two vectors: key and query vector\n",
    "1. query: what am i looking for?\n",
    "2. key: what do i contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c9b644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "\n",
    "torch.manual_seed(2026)\n",
    "B, T, C = 4, 8, 32 # batch size, time steps, channels\n",
    "x = torch.randn(B, T, C) # private information to an individual token\n",
    "\n",
    "# single head performer self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# No communication across batches\n",
    "# compute attention scores\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # (B, T, 16) # value communicated for this x\n",
    "out = wei @ v # (#B, T, T) @ (B, T, 16) --> (B, T, 16)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae49d08",
   "metadata": {},
   "source": [
    "1. Self-attention: key, query, value come from same source --> x\n",
    "2. Cross-attention: queries can be from x, key, value can come from encoder block (not from x). Separate source of information for K, Q, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564f880",
   "metadata": {},
   "source": [
    "In encoder-style models, this masking step (`wei = wei.masked_fill(tril == 0, float('-inf'))`) is not used, allowing every token to attend to all other tokens in the sequence.\n",
    "This enables bidirectional context, where each token representation is computed using both past and future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3f9266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8011, 0.1989, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6295, 0.3089, 0.0616, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4263, 0.4093, 0.0583, 0.1061, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1675, 0.0691, 0.3941, 0.0677, 0.3017, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1371, 0.1077, 0.0833, 0.1481, 0.2006, 0.3233, 0.0000, 0.0000],\n",
       "        [0.0495, 0.0313, 0.0128, 0.0485, 0.0687, 0.5069, 0.2822, 0.0000],\n",
       "        [0.2275, 0.1231, 0.0986, 0.2681, 0.0625, 0.0149, 0.0845, 0.1209]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2eafb",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "$$\n",
    "\\text{Attention}(Q, K, V)\n",
    "= \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b4288",
   "metadata": {},
   "source": [
    "d_k is the head size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50cf4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd2e0c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of k                 : 1.0863\n",
      "Variance of q                 : 0.9044\n",
      "Variance of wei               : 15.6958\n",
      "Variance of wei (adjusted)    : 0.9810\n"
     ]
    }
   ],
   "source": [
    "print(f\"Variance of k                 : {k.var().item():.4f}\")\n",
    "print(f\"Variance of q                 : {q.var().item():.4f}\")\n",
    "print(f\"Variance of wei               : {wei.var().item():.4f}\")\n",
    "print(f\"Variance of wei (adjusted)    : {(wei * head_size**-0.5).var().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122438d",
   "metadata": {},
   "source": [
    "This wei is fed into softmax, so it is important that wei is fairly diffused. \n",
    "If wei has larger variance, then softmax can converge to One-Hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3fbcd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.5937e-04, 3.1476e-05, 1.4650e-04, 3.0975e-05, 4.5887e-04, 9.9828e-01,\n",
       "        1.7238e-04, 1.2519e-04])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(wei[0][0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3003673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3482e-13, 9.8840e-19, 4.6382e-16, 9.2687e-19, 4.4644e-14, 1.0000e+00,\n",
       "        8.8904e-16, 2.4736e-16])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(wei[0][0] / head_size**-0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02af91",
   "metadata": {},
   "source": [
    "### Two ideas help in optimizing learning in deep learning networks\n",
    "1. Skip-connections (residuals)\n",
    "2. LayerNorm (similar to BatchMore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "093ec1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: \n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f0d1924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a1cca49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e5947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
