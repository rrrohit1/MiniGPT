{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efbedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-17 05:22:45--  https://huggingface.co/datasets/aaru2330/Mahabharath/resolve/main/Mahabharata.txt\n",
      "Resolving huggingface.co (huggingface.co)... 13.225.5.26, 13.225.5.100, 13.225.5.95, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.225.5.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6612ca8cdb728ea967e5b1d8/2b7db1d108c8a112055a34e8fa2a3685eff97a7e1c604df2b75c4fdec8037e8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260117T052245Z&X-Amz-Expires=3600&X-Amz-Signature=704f11be7e42964e1f151b61975a85a8a5d991c92ef7125207377c4bc524039f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mahabharata.txt%3B+filename%3D%22Mahabharata.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1768630965&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2ODYzMDk2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjEyY2E4Y2RiNzI4ZWE5NjdlNWIxZDgvMmI3ZGIxZDEwOGM4YTExMjA1NWEzNGU4ZmEyYTM2ODVlZmY5N2E3ZTFjNjA0ZGYyYjc1YzRmZGVjODAzN2U4YioifV19&Signature=ZoV%7E6o0ddRD7-y0yWqNKjomsRv1lCV26eQb6aofshX4lWwynqo0xg3Xegz1nrmqFWk6TquPOisC4jL%7E4kptGtyk-fSv-hKOpgdkx3cJL6msbJJAs-VdWD6rQdV0qPwJmcXUAwAeHqNgiNHQnzS1BrvBykoDUzqcmRN0bJMQ903YCWNKE1SRyX4jMZ6RA518ZexIw7qmHaNgR8CDfDoQAgFB8adJBdtT-cOIMiMfUMUt93h6pGf-RU3R03mqgrCzNzxrMnUTdoDsfymMqORa%7EQirHdBLubFKtiMdn8MYngq%7EyCxDmBUSn8hH5b49--NGORyDm160yXWNZwCXGoz0yhg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2026-01-17 05:22:45--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6612ca8cdb728ea967e5b1d8/2b7db1d108c8a112055a34e8fa2a3685eff97a7e1c604df2b75c4fdec8037e8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260117T052245Z&X-Amz-Expires=3600&X-Amz-Signature=704f11be7e42964e1f151b61975a85a8a5d991c92ef7125207377c4bc524039f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mahabharata.txt%3B+filename%3D%22Mahabharata.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1768630965&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2ODYzMDk2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjEyY2E4Y2RiNzI4ZWE5NjdlNWIxZDgvMmI3ZGIxZDEwOGM4YTExMjA1NWEzNGU4ZmEyYTM2ODVlZmY5N2E3ZTFjNjA0ZGYyYjc1YzRmZGVjODAzN2U4YioifV19&Signature=ZoV%7E6o0ddRD7-y0yWqNKjomsRv1lCV26eQb6aofshX4lWwynqo0xg3Xegz1nrmqFWk6TquPOisC4jL%7E4kptGtyk-fSv-hKOpgdkx3cJL6msbJJAs-VdWD6rQdV0qPwJmcXUAwAeHqNgiNHQnzS1BrvBykoDUzqcmRN0bJMQ903YCWNKE1SRyX4jMZ6RA518ZexIw7qmHaNgR8CDfDoQAgFB8adJBdtT-cOIMiMfUMUt93h6pGf-RU3R03mqgrCzNzxrMnUTdoDsfymMqORa%7EQirHdBLubFKtiMdn8MYngq%7EyCxDmBUSn8hH5b49--NGORyDm160yXWNZwCXGoz0yhg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 108.158.251.36, 108.158.251.86, 108.158.251.17, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|108.158.251.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14921157 (14M) [text/plain]\n",
      "Saving to: ‘Mahabharata.txt.2’\n",
      "\n",
      "Mahabharata.txt.2   100%[===================>]  14.23M  71.3MB/s    in 0.2s    \n",
      "\n",
      "2026-01-17 05:22:45 (71.3 MB/s) - ‘Mahabharata.txt.2’ saved [14921157/14921157]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the text\n",
    "!wget https://huggingface.co/datasets/aaru2330/Mahabharath/resolve/main/Mahabharata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f925e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file\n",
    "with open('Mahabharata.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bf9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of dataset in characters: 14921047\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caedbd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Complete Mahabharata in English\n",
      "The Mahabharata\n",
      "of\n",
      "Krishna-Dwaipayana Vyasa\n",
      "\n",
      "BOOK 1\n",
      "ADI PARVA\n",
      "Translated into English Prose from the Original Sanskrit Text by Kisari Mohan Ganguli [1883-1896]\n",
      "Scanned at sacred-texts.com, 2003. Proofed at Distributed Proofing, Juliet Sutherland, Project Manager. Additional proofing\n",
      "and formatting at sacred-texts.com, by J. B. Hare.\n",
      "TRANSLATOR'S PREFACE\n",
      "The object of a translator should ever be to hold the mirror upto his author. That being so, his chief duty is to represent so far as\n",
      "practicable the manner in which his author's ideas have been expressed, retaining if possible at the sacrifice of idiom and taste\n",
      "all the peculiarities of his author's imagery and of language as well. In regard to translations from the Sanskrit, nothing is easier\n",
      "than to dish up Hindu ideas, so as to make them agreeable to English taste. But the endeavour of the present translator has been\n",
      "to give in the following pages as literal a rendering as possible of the great wo\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfea7b",
   "metadata": {},
   "source": [
    "# Tokenzing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddac8610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\f !\"#&'(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz—\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Let observe the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92545d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 65, 2, 57, 74, 66, 77, 70]\n",
      "hi arjun\n"
     ]
    }
   ],
   "source": [
    "# Mapping characters into integers for encoding/decoding\n",
    "# character level tokenizers\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encode: string --> list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode: list of integers --> string\n",
    "\n",
    "print(encode(\"hi arjun\"))\n",
    "print(decode(encode(\"hi arjun\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da11c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cea3d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MiniGPT/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14921047]) <built-in method type of Tensor object at 0x75d88d8be6c0>\n",
      "tensor([74, 67,  2,  ...,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the entire text using pytorch\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[1000:]) # GPT view of the 1000 chars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba26aa",
   "metadata": {},
   "source": [
    "# Training the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589dfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/validation sets\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9682c",
   "metadata": {},
   "source": [
    "Block Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "262cc2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([45, 64, 61,  2, 28, 71, 69, 72, 68])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the training example length sent to the model \n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d6642",
   "metadata": {},
   "source": [
    "In this single block of 9 characters, we have 8 individual examples packed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7645b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([45]), the target is: 64\n",
      "when the input is tensor([45, 64]), the target is: 61\n",
      "when the input is tensor([45, 64, 61]), the target is: 2\n",
      "when the input is tensor([45, 64, 61,  2]), the target is: 28\n",
      "when the input is tensor([45, 64, 61,  2, 28]), the target is: 71\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71]), the target is: 69\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71, 69]), the target is: 72\n",
      "when the input is tensor([45, 64, 61,  2, 28, 71, 69, 72]), the target is: 68\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when the input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e765514",
   "metadata": {},
   "source": [
    "Batch Sizes for GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af56ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[65, 70, 60, 77, 68, 63, 61,  2],\n",
      "        [ 2, 77, 70, 76, 71,  2, 64, 65],\n",
      "        [79, 57, 74, 74, 65, 71, 74, 75],\n",
      "        [61, 60, 12,  2, 45, 64, 61,  2]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70, 60, 77, 68, 63, 61,  2, 65],\n",
      "        [77, 70, 76, 71,  2, 64, 65, 69],\n",
      "        [57, 74, 74, 65, 71, 74, 75,  2],\n",
      "        [60, 12,  2, 45, 64, 61,  2, 69]])\n",
      "----------\n",
      "When the input is: [65], the target is: 70\n",
      "When the input is: [65, 70], the target is: 60\n",
      "When the input is: [65, 70, 60], the target is: 77\n",
      "When the input is: [65, 70, 60, 77], the target is: 68\n",
      "When the input is: [65, 70, 60, 77, 68], the target is: 63\n",
      "When the input is: [65, 70, 60, 77, 68, 63], the target is: 61\n",
      "When the input is: [65, 70, 60, 77, 68, 63, 61], the target is: 2\n",
      "When the input is: [65, 70, 60, 77, 68, 63, 61, 2], the target is: 65\n",
      "When the input is: [2], the target is: 77\n",
      "When the input is: [2, 77], the target is: 70\n",
      "When the input is: [2, 77, 70], the target is: 76\n",
      "When the input is: [2, 77, 70, 76], the target is: 71\n",
      "When the input is: [2, 77, 70, 76, 71], the target is: 2\n",
      "When the input is: [2, 77, 70, 76, 71, 2], the target is: 64\n",
      "When the input is: [2, 77, 70, 76, 71, 2, 64], the target is: 65\n",
      "When the input is: [2, 77, 70, 76, 71, 2, 64, 65], the target is: 69\n",
      "When the input is: [79], the target is: 57\n",
      "When the input is: [79, 57], the target is: 74\n",
      "When the input is: [79, 57, 74], the target is: 74\n",
      "When the input is: [79, 57, 74, 74], the target is: 65\n",
      "When the input is: [79, 57, 74, 74, 65], the target is: 71\n",
      "When the input is: [79, 57, 74, 74, 65, 71], the target is: 74\n",
      "When the input is: [79, 57, 74, 74, 65, 71, 74], the target is: 75\n",
      "When the input is: [79, 57, 74, 74, 65, 71, 74, 75], the target is: 2\n",
      "When the input is: [61], the target is: 60\n",
      "When the input is: [61, 60], the target is: 12\n",
      "When the input is: [61, 60, 12], the target is: 2\n",
      "When the input is: [61, 60, 12, 2], the target is: 45\n",
      "When the input is: [61, 60, 12, 2, 45], the target is: 64\n",
      "When the input is: [61, 60, 12, 2, 45, 64], the target is: 61\n",
      "When the input is: [61, 60, 12, 2, 45, 64, 61], the target is: 2\n",
      "When the input is: [61, 60, 12, 2, 45, 64, 61, 2], the target is: 69\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2026)\n",
    "batch_size = 4 # independent sequences processed parallely\n",
    "block_size = 8 # Maximum context length of predicitons\n",
    "\n",
    "def get_batch(split):\n",
    "    # generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----------')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, : t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the input is: {context.tolist()}, the target is: {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba9e56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65, 70, 60, 77, 68, 63, 61,  2],\n",
      "        [ 2, 77, 70, 76, 71,  2, 64, 65],\n",
      "        [79, 57, 74, 74, 65, 71, 74, 75],\n",
      "        [61, 60, 12,  2, 45, 64, 61,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a0deb",
   "metadata": {},
   "source": [
    "# Let us start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607676cb",
   "metadata": {},
   "source": [
    "###  Defining the neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bb815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 84])\n",
      "tensor(5.2340, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "wMQ.Fq\fUtpuurDP4Xotw6?qF—371k!,5A-—Q?(4OxpxZRhuFTeOup4Ec]FBzc_Dxe`Dh9kvsYN'Tn(?bw9BqWEFZ\fdhu-—C)evYj\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BiGramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        # Basically we are passing the input idx to get the logits for next token prediction\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) = (batch_size, time = block_size, channel = vocab_size)\n",
    "\n",
    "        if targets is None: # If no targets provided, we are in generation mode\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape the logits and targets to calculate the loss as CrossEntropyLoss expects (N, C) and (N,) shape respectively\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, C) # last element in time dimension\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) # single prediciton in each batch\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "m = BiGramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # [0] to get the first batch element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5b048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
